{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image Quality\n",
    "- Brisque - https://pypi.org/project/brisque/\n",
    "- image-quality 1.2.7 - https://pypi.org/project/image-quality/\n",
    "- NIMA - https://github.com/yunxiaoshi/Neural-IMage-Assessment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a18e2530f24e2323"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import time\n",
    "import imquality.brisque\n",
    "import PIL.Image\n",
    "from brisque import BRISQUE\n",
    "from skimage import io\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import piq\n",
    "from skimage.io import imread\n",
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import skimage\n",
    "import cv2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:21:31.870335175Z",
     "start_time": "2023-09-29T16:21:30.283627787Z"
    }
   },
   "id": "627e6abbf72e20e3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "path1 = '/home/lukas/Bakalářka/images/Ples/fotokoutek/foto0001.jpg'\n",
    "img1 = io.imread(path1)\n",
    "path2 = '/home/lukas/Bakalářka/images/Ples/fotokoutek/foto0002.jpg'\n",
    "img2 = io.imread(path2)\n",
    "path3 = '/home/lukas/Bakalářka/images/Ples/fotokoutek/foto0004.jpg'\n",
    "img3 = io.imread(path3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T12:53:18.706297192Z",
     "start_time": "2023-09-21T12:53:18.605608810Z"
    }
   },
   "id": "589d302eedde2d05"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 2.7007605797649576\n",
      "TIME - 0.80 s\n"
     ]
    }
   ],
   "source": [
    "### BRISQUE - technical quality only\n",
    "time1 = time.perf_counter()\n",
    "obj = BRISQUE(url=False)\n",
    "print(\"SCORE:\",obj.score(img3)/10)\n",
    "time2 = time.perf_counter()\n",
    "print(f\"TIME - {time2-time1:0.2f} s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:39:46.462145829Z",
     "start_time": "2023-09-13T14:39:45.683127279Z"
    }
   },
   "id": "d6dc292c0039785"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 2.587213443716152\n",
      "TIME - 5.87 s\n"
     ]
    }
   ],
   "source": [
    "### imquality - technical quality only\n",
    "#  - implementation of brisque \n",
    "#  - very slow\n",
    "time1 = time.perf_counter()\n",
    "print(\"SCORE:\",imquality.brisque.score(PIL.Image.open(path3))/10)\n",
    "time2 = time.perf_counter()\n",
    "print(f\"TIME - {time2-time1:0.2f} s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:39:52.334310735Z",
     "start_time": "2023-09-13T14:39:46.446094784Z"
    }
   },
   "id": "246b8a78b79cf3a1"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN SCORE: 6.29 STD: 1.34\n",
      "TIME - 2.18 s\n"
     ]
    }
   ],
   "source": [
    "### NIMA - aesthetic and technical quality depending on training database\n",
    "from main import prepare_model\n",
    "time1 = time.perf_counter()\n",
    "model,device = prepare_model(\"epoch-82.pth\") # path to model\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "imt = test_transform(PIL.Image.open(path3))\n",
    "imt = imt.unsqueeze(dim=0)\n",
    "imt = imt.to(device)\n",
    "with torch.no_grad():\n",
    "    out_f, out_class = model(imt)\n",
    "out_class = out_class.view(10, 1)\n",
    "mean, std = 0.0, 0.0\n",
    "for j, e in enumerate(out_class, 1):\n",
    "    mean += j * e\n",
    "for k, e in enumerate(out_class, 1):\n",
    "    std += e * (k - mean) ** 2\n",
    "std = std ** 0.5\n",
    "print(f\"MEAN SCORE: {float(mean):0.2f} STD: {float(std):0.2f}\")\n",
    "time2 = time.perf_counter()\n",
    "print(f\"TIME - {time2-time1:0.2f} s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:39:54.525699423Z",
     "start_time": "2023-09-13T14:39:52.328333795Z"
    }
   },
   "id": "6f3fe674dee82b5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image Similarity\n",
    "- PIQ - https://github.com/photosynthesis-team/piq --> different tools and metrics to find similar images\n",
    "- Skimage - https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity\n",
    "- SIFT - https://github.com/adumrewal/SIFTImageSimilarity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60c8b08dbc76ad9"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller number means more similar images\n",
      "DISTS: 0.0581\n",
      "DISTS: 0.2703\n",
      "TIME - 5.33 s\n"
     ]
    }
   ],
   "source": [
    "# PIQ DISTS function\n",
    "# - problem with different resolutions --> after resizing too much information lost and inaccurate\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "time1 = time.perf_counter()\n",
    "x = torch.tensor(imread('~/Bakalářka/images/Ples/fotokoutek/foto0001.jpg')).permute(2, 0, 1)[None, ...] / 255.\n",
    "y = torch.tensor(imread('~/Bakalářka/images/Ples/fotokoutek/foto0002.jpg')).permute(2, 0, 1)[None, ...] / 255.\n",
    "z = torch.tensor(imread('~/Bakalářka/images/Ples/fotokoutek/foto0005.jpg')).permute(2, 0, 1)[None, ...] / 255.\n",
    "\n",
    "dists_loss_1 = piq.DISTS(reduction='none')(x, y)\n",
    "dists_loss_2 = piq.DISTS(reduction='none')(x, z)\n",
    "print(\"Smaller number means more similar images\")\n",
    "print(f\"DISTS: {dists_loss_1.item():0.4f}\")\n",
    "print(f\"DISTS: {dists_loss_2.item():0.4f}\")\n",
    "time2 = time.perf_counter()\n",
    "print(f\"TIME - {time2-time1:0.2f} s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:39:59.861214301Z",
     "start_time": "2023-09-13T14:39:54.524532237Z"
    }
   },
   "id": "b9792c6db7ecea"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND shape: [799, 799]\n",
      "Preparing images...\n",
      "Images prepared\n",
      "TIME to process images - 0.62 s\n",
      "TIME to get all similar - 1.08 s\n",
      "foto0001.jpg  most similar img is foto0002.jpg with the score of 0.7743849043067754\n",
      "foto0002.jpg  most similar img is foto0001.jpg with the score of 0.7743849043067754\n",
      "foto0003.jpg  most similar img is foto0001.jpg with the score of 0.7407976082617311\n",
      "foto0004.jpg  doesnt have similar image in set\n",
      "foto0005.jpg  doesnt have similar image in set\n"
     ]
    }
   ],
   "source": [
    "### Skimage\n",
    "#   - outdated implementation with square complexity \n",
    "#   - high memory and cpu usage for mediocre results \n",
    "#   - faster than PIQ DISTS function\n",
    "\n",
    "pth_images = '/home/lukas/Bakalářka/images/Ples/fotokoutek'\n",
    "SIM_LIM = 5  # limiting the number of images for fast example\n",
    "SIM_THRESHOLD = 0.7  # threshold for similarity\n",
    "\n",
    "def imread_reshape(path_images):\n",
    "    min_w = float('inf')\n",
    "    min_h = float('inf')\n",
    "    num = 0\n",
    "    i = 0\n",
    "    lst = os.listdir(path_images)\n",
    "    lst.sort()\n",
    "    for image in lst:\n",
    "        if i >= SIM_LIM:\n",
    "            break\n",
    "        i+=1\n",
    "        temp = io.imread(os.path.join(path_images, image))\n",
    "        if temp is not None:\n",
    "            num += 1\n",
    "            if min_w > temp.shape[0]:\n",
    "                min_w = temp.shape[0]\n",
    "            if min_h > temp.shape[1]:\n",
    "                min_h = temp.shape[1]\n",
    "    shape = [min_w, min_h]\n",
    "\n",
    "    print('FOUND shape:', shape)\n",
    "    print('Preparing images...')\n",
    "\n",
    "    images = None\n",
    "    i = 0\n",
    "    for image in lst:\n",
    "        if i >= SIM_LIM:\n",
    "            break\n",
    "        i+=1\n",
    "        temp = torch.tensor(io.imread(os.path.join(path_images, image)))/255.\n",
    "        if temp is not None:\n",
    "            temp = skimage.transform.resize_local_mean(temp, output_shape=shape)\n",
    "            temp = skimage.color.rgb2gray(temp)\n",
    "            temp = np.expand_dims(temp, 2)\n",
    "            temp = np.rollaxis(temp, 2, 0).astype('f2')\n",
    "            if images is None:\n",
    "                images = temp\n",
    "            else:\n",
    "                images = np.append(images,temp,0).astype('f2')\n",
    "    print('Images prepared')\n",
    "    return images, num, lst\n",
    "\n",
    "time1 = time.perf_counter()\n",
    "ic, img_num, img_lst = imread_reshape(pth_images)\n",
    "time2 = time.perf_counter()\n",
    "\n",
    "print(f\"TIME to process images - {time2-time1:0.2f} s\")\n",
    "\n",
    "sim_list = [[0 for i in range(img_num)] for j in range(img_num)]\n",
    "\n",
    "\n",
    "time1 = time.perf_counter()\n",
    "for i in range(img_num):\n",
    "    for j in range(img_num):\n",
    "        sim_list[i][j] = skimage.metrics.structural_similarity(ic[:][:][:][i], ic[:][:][:][j], win_size=3, data_range= 2)\n",
    "        print(' {0}/{1}'.format(i*img_num+(j+1),img_num*img_num),end='\\r')\n",
    "time2 = time.perf_counter()\n",
    "print(f\"TIME to get all similar - {time2-time1:0.2f} s\")\n",
    "\n",
    "for i in range(len(sim_list[0])):\n",
    "    most_similar = 0\n",
    "    most_idx = 0\n",
    "    for j in range(len(sim_list[0])):\n",
    "        if i == j:\n",
    "            continue\n",
    "        if sim_list[i][j] > most_similar:\n",
    "            most_similar =  sim_list[i][j]\n",
    "            most_idx = j\n",
    "    if most_similar > SIM_THRESHOLD:\n",
    "        print(img_lst[i],' most similar img is',img_lst[most_idx], 'with the score of', most_similar)\n",
    "    else:\n",
    "        print(img_lst[i],' doesnt have similar image in set')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T14:40:01.619321367Z",
     "start_time": "2023-09-13T14:39:59.870010940Z"
    }
   },
   "id": "81c22a6737c9075a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [1], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m SIM_LIM \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m\n\u001B[1;32m      9\u001B[0m lst \u001B[38;5;241m=\u001B[39m []  \u001B[38;5;66;03m# list of image file names to process\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241m.\u001B[39mscandir(pth):\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mname\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from main import calculate_score,calculate_matches,compute_SIFT,image_resize,prepare_img_list\n",
    "\n",
    "pth = '/home/lukas/Bakalářka/images/Ples/fotokoutek'\n",
    "nbrs = 5\n",
    "SIM_LIM = 20\n",
    "lst,num = prepare_img_list(pth)\n",
    "if num > SIM_LIM:\n",
    "    num = SIM_LIM\n",
    "\n",
    "features={} # keypoints and descriptors\n",
    "sim_list = [] # list to store results\n",
    "time1 = time.perf_counter()\n",
    "for i in range(num):\n",
    "    img = image_resize(cv2.imread(os.path.join(pth,lst[i])))\n",
    "    keypoints, descriptors = compute_SIFT(img)\n",
    "    features[i] = (keypoints,descriptors)\n",
    "for i in range(num):\n",
    "    keypoints_i, descriptors_i = features[i]\n",
    "    for j in range(max(0,i-nbrs),min(num,i+nbrs+1)):\n",
    "        if i>=j:\n",
    "            continue\n",
    "        keypoints_j, descriptors_j = features[j]\n",
    "        matches = calculate_matches(descriptors_i, descriptors_j)\n",
    "        score = calculate_score(len(matches), len(keypoints_i), len(keypoints_j))\n",
    "        print(\"IMAGES:\",lst[i], lst[j],\"SIMILARITY SCORE:\",score)\n",
    "time2 = time.perf_counter()\n",
    "print(f\"TIME - {time2-time1:0.2f} s\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T11:23:54.384879962Z",
     "start_time": "2023-09-21T11:23:51.546953060Z"
    }
   },
   "id": "3ef6579db08ebcf9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image Content\n",
    "- VGG16 - https://towardsdatascience.com/how-to-use-a-pre-trained-model-vgg-for-image-classification-8dd7c4a4a517\n",
    "- Bag of words approach from similarity search results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cb00823e63c04a0"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('n03720891', 'maraca', 0.22046313)]]\n",
      "[[('n03720891', 'maraca', 0.12685233)]]\n",
      "[[('n04418357', 'theater_curtain', 0.1669903)]]\n",
      "[[('n04418357', 'theater_curtain', 0.3567077)]]\n",
      "[[('n04418357', 'theater_curtain', 0.42902043)]]\n",
      "[[('n04418357', 'theater_curtain', 0.3086879)]]\n",
      "[[('n04418357', 'theater_curtain', 0.5442761)]]\n",
      "[[('n04418357', 'theater_curtain', 0.4918293)]]\n",
      "[[('n04418357', 'theater_curtain', 0.61957985)]]\n",
      "[[('n04418357', 'theater_curtain', 0.77726114)]]\n",
      "[[('n04418357', 'theater_curtain', 0.87706)]]\n",
      "[[('n04418357', 'theater_curtain', 0.80182195)]]\n",
      "[[('n04418357', 'theater_curtain', 0.5945882)]]\n",
      "[[('n04418357', 'theater_curtain', 0.5792202)]]\n",
      "[[('n03866082', 'overskirt', 0.13092667)]]\n",
      "[[('n03450230', 'gown', 0.17325132)]]\n",
      "[[('n04418357', 'theater_curtain', 0.35916135)]]\n",
      "[[('n04350905', 'suit', 0.4481231)]]\n",
      "[[('n04418357', 'theater_curtain', 0.25696555)]]\n",
      "[[('n02883205', 'bow_tie', 0.49764696)]]\n",
      "[[('n04418357', 'theater_curtain', 0.2750732)]]\n",
      "[[('n02883205', 'bow_tie', 0.7261428)]]\n",
      "[[('n02883205', 'bow_tie', 0.2566041)]]\n",
      "[[('n04418357', 'theater_curtain', 0.29767996)]]\n",
      "[[('n04418357', 'theater_curtain', 0.18858936)]]\n",
      "[[('n03450230', 'gown', 0.2413577)]]\n",
      "[[('n04418357', 'theater_curtain', 0.8521128)]]\n",
      "[[('n04350905', 'suit', 0.39552107)]]\n",
      "[[('n04350905', 'suit', 0.3786114)]]\n",
      "[[('n04296562', 'stage', 0.39507368)]]\n",
      "[[('n04418357', 'theater_curtain', 0.78159046)]]\n",
      "[[('n04418357', 'theater_curtain', 0.7324758)]]\n",
      "[[('n10148035', 'groom', 0.36047554)]]\n",
      "[[('n10148035', 'groom', 0.1768921)]]\n",
      "[[('n04418357', 'theater_curtain', 0.74152225)]]\n",
      "[[('n04418357', 'theater_curtain', 0.62257457)]]\n",
      "[[('n03534580', 'hoopskirt', 0.89293164)]]\n",
      "[[('n03534580', 'hoopskirt', 0.8813575)]]\n",
      "[[('n04418357', 'theater_curtain', 0.8501157)]]\n",
      "[[('n04418357', 'theater_curtain', 0.7711112)]]\n",
      "[[('n03534580', 'hoopskirt', 0.8460307)]]\n",
      "[[('n04418357', 'theater_curtain', 0.37133962)]]\n",
      "[[('n04418357', 'theater_curtain', 0.38865593)]]\n",
      "[[('n03534580', 'hoopskirt', 0.5777216)]]\n",
      "[[('n03866082', 'overskirt', 0.40775543)]]\n",
      "[[('n03534580', 'hoopskirt', 0.64565915)]]\n",
      "[[('n03534580', 'hoopskirt', 0.75855356)]]\n",
      "[[('n04296562', 'stage', 0.21483317)]]\n",
      "[[('n04418357', 'theater_curtain', 0.78516406)]]\n",
      "[[('n04418357', 'theater_curtain', 0.77542126)]]\n",
      "[[('n04418357', 'theater_curtain', 0.8534319)]]\n",
      "[[('n02787622', 'banjo', 0.6021745)]]\n",
      "[[('n04418357', 'theater_curtain', 0.50048727)]]\n",
      "[[('n10148035', 'groom', 0.32862803)]]\n",
      "[[('n04418357', 'theater_curtain', 0.7648563)]]\n",
      "[[('n04418357', 'theater_curtain', 0.7469007)]]\n",
      "[[('n04418357', 'theater_curtain', 0.47521478)]]\n",
      "[[('n04418357', 'theater_curtain', 0.7772833)]]\n",
      "[[('n04350905', 'suit', 0.3406454)]]\n",
      "[[('n04350905', 'suit', 0.28863862)]]\n",
      "[[('n10148035', 'groom', 0.8035807)]]\n",
      "[[('n10148035', 'groom', 0.57344234)]]\n",
      "[[('n10148035', 'groom', 0.23363444)]]\n",
      "[[('n03866082', 'overskirt', 0.26366782)]]\n",
      "[[('n04418357', 'theater_curtain', 0.48943076)]]\n",
      "[[('n04418357', 'theater_curtain', 0.7720258)]]\n",
      "[[('n04418357', 'theater_curtain', 0.62123156)]]\n",
      "[[('n10148035', 'groom', 0.2781652)]]\n",
      "[[('n10148035', 'groom', 0.25946832)]]\n",
      "[[('n04418357', 'theater_curtain', 0.9504591)]]\n",
      "[[('n04418357', 'theater_curtain', 0.6553853)]]\n",
      "[[('n04418357', 'theater_curtain', 0.4975345)]]\n",
      "[[('n03450230', 'gown', 0.36927623)]]\n",
      "[[('n04418357', 'theater_curtain', 0.9011801)]]\n",
      "[[('n04418357', 'theater_curtain', 0.7947701)]]\n",
      "[[('n04418357', 'theater_curtain', 0.19787368)]]\n",
      "[[('n04296562', 'stage', 0.6249248)]]\n",
      "[[('n04418357', 'theater_curtain', 0.47534332)]]\n",
      "[[('n03770439', 'miniskirt', 0.29676637)]]\n",
      "[[('n04350905', 'suit', 0.6246116)]]\n",
      "[[('n04350905', 'suit', 0.32370687)]]\n",
      "[[('n03450230', 'gown', 0.39388174)]]\n",
      "[[('n04418357', 'theater_curtain', 0.94073886)]]\n",
      "[[('n04296562', 'stage', 0.46338475)]]\n",
      "[[('n04418357', 'theater_curtain', 0.8616468)]]\n",
      "[[('n04418357', 'theater_curtain', 0.84079915)]]\n",
      "[[('n04418357', 'theater_curtain', 0.23345935)]]\n",
      "[[('n04418357', 'theater_curtain', 0.59641707)]]\n",
      "[[('n04418357', 'theater_curtain', 0.60248077)]]\n",
      "[[('n04418357', 'theater_curtain', 0.5044135)]]\n",
      "[[('n04418357', 'theater_curtain', 0.5205453)]]\n",
      "[[('n04418357', 'theater_curtain', 0.6264475)]]\n",
      "[[('n04418357', 'theater_curtain', 0.88246506)]]\n",
      "[[('n04418357', 'theater_curtain', 0.3370451)]]\n",
      "[[('n04296562', 'stage', 0.31038886)]]\n",
      "[[('n04418357', 'theater_curtain', 0.363768)]]\n",
      "[[('n04418357', 'theater_curtain', 0.3067052)]]\n",
      "[[('n04418357', 'theater_curtain', 0.88601476)]]\n",
      "[[('n04350905', 'suit', 0.35647306)]]\n",
      "[[('n04418357', 'theater_curtain', 0.6866859)]]\n",
      "[[('n02787622', 'banjo', 0.47247854)]]\n",
      "[[('n04296562', 'stage', 0.2668486)]]\n",
      "[[('n04418357', 'theater_curtain', 0.6570848)]]\n",
      "[[('n04418357', 'theater_curtain', 0.9234301)]]\n",
      "[[('n04418357', 'theater_curtain', 0.9201089)]]\n",
      "[[('n04418357', 'theater_curtain', 0.88076127)]]\n",
      "[[('n04418357', 'theater_curtain', 0.9555499)]]\n",
      "[[('n04418357', 'theater_curtain', 0.9684147)]]\n",
      "[[('n04418357', 'theater_curtain', 0.752179)]]\n",
      "[[('n04418357', 'theater_curtain', 0.70917207)]]\n",
      "[[('n04418357', 'theater_curtain', 0.5791666)]]\n",
      "[[('n03866082', 'overskirt', 0.520839)]]\n",
      "[[('n03866082', 'overskirt', 0.43146384)]]\n",
      "[[('n03450230', 'gown', 0.23503669)]]\n",
      "[[('n03450230', 'gown', 0.32206365)]]\n",
      "[[('n04296562', 'stage', 0.30681965)]]\n",
      "[[('n04418357', 'theater_curtain', 0.6317725)]]\n",
      "[[('n04418357', 'theater_curtain', 0.35598096)]]\n",
      "[[('n04418357', 'theater_curtain', 0.9717472)]]\n",
      "[[('n03877472', 'pajama', 0.3554018)]]\n",
      "[[('n04418357', 'theater_curtain', 0.21558905)]]\n",
      "[[('n04418357', 'theater_curtain', 0.7419451)]]\n",
      "[[('n04418357', 'theater_curtain', 0.71280664)]]\n",
      "[[('n10148035', 'groom', 0.37211752)]]\n",
      "[[('n02804610', 'bassoon', 0.30889812)]]\n",
      "[[('n04296562', 'stage', 0.3453279)]]\n",
      "[[('n03866082', 'overskirt', 0.5591545)]]\n",
      "[[('n03866082', 'overskirt', 0.37185726)]]\n",
      "[[('n04418357', 'theater_curtain', 0.5910529)]]\n",
      "[[('n10148035', 'groom', 0.24207546)]]\n",
      "[[('n04418357', 'theater_curtain', 0.4637917)]]\n",
      "[[('n04418357', 'theater_curtain', 0.5015686)]]\n",
      "[[('n04418357', 'theater_curtain', 0.54108083)]]\n",
      "[[('n04039381', 'racket', 0.8493967)]]\n",
      "[[('n04418357', 'theater_curtain', 0.83816665)]]\n",
      "[[('n04296562', 'stage', 0.5362113)]]\n",
      "TIME - 55.90 s\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from keras.applications.vgg16 import preprocess_input,decode_predictions,VGG16\n",
    "from keras import preprocessing\n",
    "from main import prepare_img_list\n",
    "\n",
    "class_model = VGG16(weights='imagenet')\n",
    "SIM_LIM = 500\n",
    "\n",
    "img_path = \"/home/lukas/Bakalářka/photo_culling/images/Ples/fotokoutek\"\n",
    "content_path = \"image_content_fotokoutek.json\"\n",
    "lst,_ = prepare_img_list(img_path)\n",
    "\n",
    "time1 = time.perf_counter()\n",
    "for i, p in enumerate(lst):\n",
    "    if i >= SIM_LIM:\n",
    "        break\n",
    "    img = preprocessing.image.load_img(os.path.join(img_path,p),color_mode='rgb', target_size=(224, 224))\n",
    "    t = preprocessing.image.img_to_array(img)\n",
    "    t = np.expand_dims(t, axis=0)\n",
    "    t = preprocess_input(t)\n",
    "    f = class_model.predict(t)\n",
    "    print(decode_predictions(f,top=1))\n",
    "time2 = time.perf_counter()\n",
    "print(f\"TIME - {time2-time1:0.2f} s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T16:45:56.975958775Z",
     "start_time": "2023-09-29T16:44:57.368362705Z"
    }
   },
   "id": "c055d2d52527e75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "59a246fe76283a9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
