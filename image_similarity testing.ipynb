{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image Similarity\n",
    "- Skimage (SSIM) - https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity\n",
    "- PIQ (DISTS) - https://github.com/photosynthesis-team/piq\n",
    "- SIFT - https://github.com/adumrewal/SIFTImageSimilarity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63cc0f2f30f473eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For finding similar or duplicate photos  we need to extract a metric (similarity score) from a pair of images and then decide to what extent we consider images similar based on this number. There are two ways to get the similarity score. First one is image-based, where we compute the distance between pair of images after resizing them. Second one is distribution-based, where we extract features from images and compute the distance between pair of feature distributions.\n",
    "\n",
    "For testing we created small folder with image and its various augmentations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1b06c0ac0028d44"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import time, os\n",
    "image_path = '/home/lukas/Bakalářka/photo_culling/images/testing'\n",
    "img_list = []  # list of image file names to process\n",
    "for path in os.scandir(image_path):\n",
    "    if path.is_file():\n",
    "        if path.name.endswith(\".jpg\"):\n",
    "            img_list += [path.name]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T14:10:07.443233934Z",
     "start_time": "2023-09-24T14:10:07.402702624Z"
    }
   },
   "id": "f146692b2e5bdf9"
  },
  {
   "cell_type": "markdown",
   "source": [
    " We tested two image-based metrics. First one is a metric called Structural Similarity (SSIM) invented in year 2004. We used implementation from skimage library.\n",
    " (see https://scikit-image.org/docs/stable/api/skimage.metrics.html#skimage.metrics.structural_similarity)\n",
    "For both image-based metrics we firstly need to reshape the images to same resolution to be able to compare the similarity. We consider two images similar if the similarity score is above 0.7."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1572cb56066aea16"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME to process images - 2.30 s\n"
     ]
    }
   ],
   "source": [
    "# need to resize images to match\n",
    "def imread_reshape(path,lst):\n",
    "    shape = [224,224]\n",
    "    images = None\n",
    "    num = 0\n",
    "    for image in lst:\n",
    "        temp = torch.tensor(io.imread(os.path.join(path, image)))/255.\n",
    "        if temp is not None:\n",
    "            num+=1\n",
    "            temp = skimage.transform.resize_local_mean(temp, output_shape=shape)\n",
    "            temp = np.expand_dims(temp, 2)\n",
    "            temp = np.rollaxis(temp, 2, 0).astype('f2')\n",
    "            temp = np.rollaxis(temp, 3, 1).astype('f2')\n",
    "            if images is None:\n",
    "                images = temp\n",
    "            else:\n",
    "                images = np.append(images,temp,0).astype('f2')\n",
    "    return images, num\n",
    "\n",
    "tic = time.perf_counter()\n",
    "images, img_num = imread_reshape(image_path,img_list)\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME to process images - {toc-tic:0.2f} s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:10:15.408599507Z",
     "start_time": "2023-09-24T15:10:13.082167661Z"
    }
   },
   "id": "7cd5667d7c548fd4"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME to get all similar - 0.15 s\n",
      "[1.    0.986 0.481 0.111 0.964 0.77 ] clear.jpg\n",
      "[0.986 1.    0.488 0.128 0.955 0.755] GaussBlur.jpg\n",
      "[0.481 0.488 1.    0.249 0.476 0.426] rotated.jpg\n",
      "[0.111 0.128 0.249 1.    0.09  0.113] inverted.jpg\n",
      "[0.964 0.955 0.476 0.09  1.    0.737] hue_shift.jpg\n",
      "[0.77  0.755 0.426 0.113 0.737 1.   ] contrast.jpg\n"
     ]
    }
   ],
   "source": [
    "import skimage, torch\n",
    "import numpy as np\n",
    "from skimage import  io\n",
    "\n",
    "sim_list = [[0 for i in range(img_num)] for j in range(img_num)]\n",
    "nbrs = 5\n",
    "tic = time.perf_counter()\n",
    "for i in range(img_num):\n",
    "    x = skimage.color.rgb2gray(images[i],channel_axis=0)\n",
    "    for j in range(img_num):\n",
    "        if i<j:\n",
    "            continue\n",
    "        y = skimage.color.rgb2gray(images[j],channel_axis=0)\n",
    "        sim_list[i][j] = skimage.metrics.structural_similarity(x, y, win_size=3, data_range= 1,sigma=1.5,use_sample_covariance=False,gaussian_weights=True)\n",
    "        sim_list[j][i] = sim_list[i][j]\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME to get all similar - {toc-tic:0.2f} s\")\n",
    "for i, line in enumerate(sim_list):\n",
    "    line = np.round(line,decimals=3)\n",
    "    print(np.trim_zeros(line),img_list[i],)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:41:55.074729703Z",
     "start_time": "2023-09-24T15:41:54.954960462Z"
    }
   },
   "id": "cebb26429c998dde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resulting 2D array shows us similarity score between each pair of images. The diagonal of ones means that the images are duplicate which is obvious since we compare image to itself. Numbers that are closer to 1 mean that the images are more similar. We can see from the results that blurring the image or shifting the color hue doesn't result in significant score dropping. Changing the contrast causes a drop that might is significant enough that for some cases the similarity might not be detected. Rotating and inverting the image causes that the similarity is not detected. The big positive is extremely fast processing of the results.\n",
    "\n",
    "The second image-based metric that we tested is called Deep Image Structure and Texture Similarity (DISTS) from the year 2020. We chose to use implementation from PyTorch Image Quality library made by Photosynthesis Team. This metric implements deep learning into the computing and as result is more accurate with significantly longer process times.\n",
    "(see https://github.com/photosynthesis-team/piq)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "401866a446cd06f9"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME to get all similar - 37.62 s\n",
      "[1.    0.959 0.817 0.573 0.8   0.793] clear.jpg\n",
      "[0.959 1.    0.795 0.558 0.782 0.771] GaussBlur.jpg\n",
      "[0.817 0.795 1.    0.609 0.72  0.751] rotated.jpg\n",
      "[0.573 0.558 0.609 1.    0.637 0.564] inverted.jpg\n",
      "[0.8   0.782 0.72  0.637 1.    0.764] hue_shift.jpg\n",
      "[0.793 0.771 0.751 0.564 0.764 1.   ] contrast.jpg\n"
     ]
    }
   ],
   "source": [
    "import piq\n",
    "import warnings\n",
    "\n",
    "sim_list = [[0 for i in range(img_num)] for j in range(img_num)]\n",
    "nbrs = 5\n",
    "tic = time.perf_counter()\n",
    "for i in range(img_num):\n",
    "    x = torch.Tensor(images[i][None,...])\n",
    "    for j in range(img_num):\n",
    "        if i<j:\n",
    "            continue\n",
    "        y = torch.Tensor(images[j][None,...])\n",
    "        temp = piq.DISTS(reduction='none')(x,y)\n",
    "        sim_list[i][j] = 1 - temp.item()\n",
    "        sim_list[j][i] = sim_list[i][j]\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME to get all similar - {toc-tic:0.2f} s\")\n",
    "for i, line in enumerate(sim_list):\n",
    "    line = np.round(line,decimals=3)\n",
    "    print(np.trim_zeros(line),img_list[i],)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T15:42:47.709399653Z",
     "start_time": "2023-09-24T15:42:10.021660598Z"
    }
   },
   "id": "6adbd529d6e45ad2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results are similar to the ones from previous metric. The main difference is that rotated image is scored much higher than in previous test which is desirable as it detects rotated images as similar. Score for inverted images is still low as it would escape the similarity detection. Overall this algorithm has better behaviour for our usage but comes at cost of significantly higher processing time.\n",
    "\n",
    "To avoid problems with rotated, inverted and scaled images escaping our detection we will have to test out algorithm that is distribution-based. In our case we chose algorithm called Scale Invariant Feature Transform (SIFT) from year 2004. Our implementation is inspired by code made by Amol Dumrewal.\n",
    "(see https://github.com/adumrewal/SIFTImageSimilarity)\n",
    "\n",
    "For this approach we consider images similar if the similarity score is higher than 0.1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c3e375168d0b6ac"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME - 1.75 s\n",
      "[1.    0.648 0.628 0.    0.695 0.247] clear.jpg\n",
      "[0.648 1.    0.539 0.    0.6   0.248] GaussBlur.jpg\n",
      "[0.628 0.539 1.    0.    0.555 0.202] rotated.jpg\n",
      "[0.    0.    0.    1.    0.    0.006] inverted.jpg\n",
      "[0.695 0.6   0.555 0.    1.    0.195] hue_shift.jpg\n",
      "[0.247 0.248 0.202 0.006 0.195 1.   ] contrast.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def compute_SIFT(image):\n",
    "    return sift.detectAndCompute(image, None)\n",
    "\n",
    "def image_resize(image):\n",
    "    max_d = 1024\n",
    "    height,width,channel = image.shape\n",
    "    aspect_ratio = width/height\n",
    "    if aspect_ratio < 1:\n",
    "        new_size = (int(max_d*aspect_ratio),max_d)\n",
    "    else:\n",
    "        new_size = (max_d,int(max_d/aspect_ratio))\n",
    "    image = cv2.resize(image,new_size)\n",
    "    return image\n",
    "\n",
    "def calculate_matches(des1, des2):\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    top_results1 = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            top_results1.append([m])\n",
    "\n",
    "    matches = bf.knnMatch(des2, des1, k=2)\n",
    "    top_results2 = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            top_results2.append([m])\n",
    "\n",
    "    top_results = []\n",
    "    for match1 in top_results1:\n",
    "        match1_query_index = match1[0].queryIdx\n",
    "        match1_train_index = match1[0].trainIdx\n",
    "\n",
    "        for match2 in top_results2:\n",
    "            match2_query_index = match2[0].queryIdx\n",
    "            match2_train_index = match2[0].trainIdx\n",
    "\n",
    "            if (match1_query_index == match2_train_index) and (match1_train_index == match2_query_index):\n",
    "                top_results.append(match1)\n",
    "    return top_results\n",
    "\n",
    "def calculate_score(matches,keypoint1,keypoint2):\n",
    "    return 100 * (matches/min(keypoint1,keypoint2))\n",
    "\n",
    "sift = cv2.SIFT_create(10000) # SIFT algorithm with number of keypoints\n",
    "bf = cv2.BFMatcher() # keypoint matcher\n",
    "\n",
    "sim_list = [[0 for i in range(img_num)] for j in range(img_num)]\n",
    "num = len(img_list)\n",
    "features={} # keypoints and descriptors\n",
    "tic = time.perf_counter()\n",
    "for i in range(num):\n",
    "    img = image_resize(cv2.imread(os.path.join(image_path,img_list[i])))\n",
    "    keypoints, descriptors = compute_SIFT(img)\n",
    "    features[i] = (keypoints,descriptors)\n",
    "for i in range(num):\n",
    "    keypoints_i, descriptors_i = features[i]\n",
    "    for j in range(max(0,i-nbrs),min(num,i+nbrs+1)):\n",
    "        if i>j:\n",
    "            continue\n",
    "        keypoints_j, descriptors_j = features[j]\n",
    "        matches = calculate_matches(descriptors_i, descriptors_j)\n",
    "        score = calculate_score(len(matches), len(keypoints_i), len(keypoints_j))\n",
    "        sim_list[i][j] = score\n",
    "        sim_list[j][i] = sim_list[i][j]\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME - {toc-tic:0.2f} s\")\n",
    "for i, line in enumerate(sim_list):\n",
    "    line = np.divide(line,100)\n",
    "    line = np.round(line,decimals=3)\n",
    "    print(line,img_list[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T16:20:41.683726147Z",
     "start_time": "2023-09-24T16:20:39.907932728Z"
    }
   },
   "id": "8f58cff0ae3e2caf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the results we can see that blurred, rotated and hue-shifted images are easily recognized as similar. For image with different contrast the similarity score is lower but still well above our threshold for what is considered similar. The inverted image is however scored as not similar at all which is behaviour that is not preferred. The speed of this algorithm is very good."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1865ccb1d68c6f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO - We will try to test those algorithm on bigger number of augmented images to help us reveal if there are any other downsides to said algorithms. For now this is our summary of the results:\n",
    "\n",
    "Overall these three algorithms have each their upsides and downsides. \n",
    "All of them were unable to recognize similarity of inverted picture. This could be solved by calculating every image as normal and inverted version, and we can then take the highest of those scores as our result. This would end up taking about twice as much processing time but for our faster algorithms it might be an effective solution. This requires more testing.\n",
    "\n",
    "The first implementation of SSIM was the fastest. This came at cost of not very precise results especially for rotated images. Considering its inaccuracy this algorithm isn't ideal for our usage.\n",
    "\n",
    "The second implementation of DISTS was the slowest. This algorithm was able to recognize similarity in all the testing images other than the inverted one. The higher accuracy is not enough to outweigh the big computing time and so this algorithm is also not usable for us.\n",
    "\n",
    "The third implementation of SIFT was slower than SSIM but still very reasonable. This algorithm had great accuracy for all images other than inverted one. This algorithm is well-balanced in terms of computing time and accuracy and will likely be the algorithm that we use. The only downside is that for this algorithm to detect inverted images as similar the solution would be harder to implement. TODO - This requires more testing. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17cc805343df9273"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
