{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image Quality\n",
    "- Brisque - https://pypi.org/project/brisque/\n",
    "- image-quality 1.2.7 - https://pypi.org/project/image-quality/\n",
    "- NIMA for aesthetic quality - https://github.com/yunxiaoshi/Neural-IMage-Assessment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f786a6e7de7ade7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For assessing image quality we have to consider two aspects. Technical and aesthetic quality of an image. Our usage involves no reference for quality assessment. We are going to look into how fast and sensitive the image quality evaluator is and evaluate which one is the most efficient for our use.\n",
    "\n",
    "# TECHNICAL QUALITY\n",
    "Firstly we are going to focus on technical quality. For technical quality we can choose from two approaches, one that is algorithmic and focused on scene statistics and the second one, which is using CNN trained on TID2013 dataset.\n",
    "\n",
    "For algorithmic approach we tested two implementations of the same algorithm called BRISQUE (Blind/referenceless image spatial quality evaluator). First implementation is library created by Rehan Guha (https://pypi.org/project/brisque/). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b81d75e55fe9ab"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import time,os\n",
    "from brisque import BRISQUE\n",
    "from skimage import io"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T12:40:52.374749416Z",
     "start_time": "2023-09-23T12:40:52.353821248Z"
    }
   },
   "id": "e30cf9e3f73ce306"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# We create list of images that we use for testing\n",
    "\n",
    "image_path = '/home/lukas/Bakalářka/photo_culling/images/testing'\n",
    "img_list = []  # list of image file names to process\n",
    "for path in os.scandir(image_path):\n",
    "    if path.is_file():\n",
    "        if path.name.endswith(\".jpg\"):\n",
    "            img_list += [path.name]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T12:40:52.716217409Z",
     "start_time": "2023-09-23T12:40:52.686959641Z"
    }
   },
   "id": "f533be1ba8b2db32"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME - 44.76 s\n",
      "['clear.jpg', 'GaussBlur.jpg', 'rotated.jpg', 'inverted.jpg', 'hue_shift.jpg', 'contrast.jpg']\n",
      "[63.439140291231155, 96.60843292281524, 63.230106082455876, 63.30271773240301, 59.23486617151738, 71.03791634023887]\n"
     ]
    }
   ],
   "source": [
    "obj = BRISQUE(url=False)\n",
    "results_BRISQUE = []\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for img in img_list:\n",
    "    x = io.imread(os.path.join(image_path,img))\n",
    "    results_BRISQUE.append(obj.score(x))\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME - {toc - tic:0.2f} s\")\n",
    "print(img_list)\n",
    "print(results_BRISQUE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T14:46:18.279101090Z",
     "start_time": "2023-09-22T14:45:33.478369276Z"
    }
   },
   "id": "85cbdd61b53f35d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this first test we can see that for this algorithm rotating and inverting has little to no effect. For shifted hue and increased contrast we can observe\n",
    "small change in quality score. And for Gaussian blur we can see the biggest quality score drop. This is expected as the technical quality should only be measured\n",
    "by pixels relation to its surroundings.\n",
    "\n",
    "Now we can try testing the second implementation of BRISQUE from image-quality library made by Ricardo Ocampo. In this implementation we need to open the images\n",
    "with Pillow image library function. This is slight downside."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2adfee4132c43ef6"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import imquality.brisque"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T14:46:18.347932051Z",
     "start_time": "2023-09-22T14:46:18.304014124Z"
    }
   },
   "id": "1f4eb56922819494"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME - 242.50 s\n",
      "['clear.jpg', 'GaussBlur.jpg', 'rotated.jpg', 'inverted.jpg', 'hue_shift.jpg', 'contrast.jpg']\n",
      "[63.76477488661598, 98.15799411945116, 63.57080624559066, 63.595836343125, 59.454852915252076, 71.33528411947336]\n"
     ]
    }
   ],
   "source": [
    "results_BRISQUE = []\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for img in img_list:\n",
    "    x = imquality.brisque.score(PIL.Image.open(os.path.join(image_path,img)))\n",
    "    results_BRISQUE.append(x)\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME - {toc - tic:0.2f} s\")\n",
    "print(img_list)\n",
    "print(results_BRISQUE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-22T14:50:20.820862811Z",
     "start_time": "2023-09-22T14:46:18.319670204Z"
    }
   },
   "id": "5263bb5080c34aa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the second test we can observe the same as in the first one. The most significant score change is in the case of blurring the image. The quality scores\n",
    "for both implementations are nearly identical.\n",
    "\n",
    "Both of these solutions are very easy-to-use, give us single score number representing the technical quality and are linear in terms of computing time. That said,\n",
    "the first implementation is around 5 times faster and doesn't require us to open images beforehand. From these factors we can already see that the first\n",
    "implementation is superior of those two.\n",
    "\n",
    "Now we can go ahead and test CNN approach to technical quality assessment.\n",
    "- For this I haven't been able to test it yet. As I struggle to build the network with pretrained weights."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0bc1cd2a4e9319"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T12:22:56.330699528Z",
     "start_time": "2023-09-23T12:22:56.320156673Z"
    }
   },
   "id": "9fe8adc961934356"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AESTHETIC QUALITY\n",
    "For aesthetics there are significantly fewer solutions as the question of aesthetics is vey subjective and so it's complicated to create an algorithmic solution.\n",
    "For this reason we are going to focus on deep learning approach with CNN trained on AVA dataset. This dataset is created with images of amateur photographers and as such they are focused on aesthetic quality to the images. The rating is trained on ratings of the public and hence it is as close to objective beauty rating as we can currently get. The output of the network is distribution of ratings that is simulating an actual distribution of ratings that people might give. From this distribution we are then able to get mean rating and the statistical deviation of the rating.\n",
    "\n",
    "We used implementation inspired by https://github.com/yunxiaoshi/Neural-IMage-Assessment. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afee0aca3102d95e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME - 2.90 s\n",
      "['clear.jpg', 'GaussBlur.jpg', 'rotated.jpg', 'inverted.jpg', 'hue_shift.jpg', 'contrast.jpg']\n",
      "[(6.243923664093018, 1.210378885269165), (11.774584770202637, 6.50901460647583), (17.910324096679688, 12.120135307312012), (24.269302368164062, 18.306550979614258), (30.483896255493164, 24.67669677734375), (36.416202545166016, 30.916540145874023)]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NIMA(nn.Module):\n",
    "    \"\"\"Neural IMage Assessment model by Google\"\"\"\n",
    "    def __init__(self, base_model, num_classes=10):\n",
    "        super(NIMA, self).__init__()\n",
    "        self.features = base_model.features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.75),\n",
    "            nn.Linear(in_features=25088, out_features=num_classes),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_f = self.features(x)\n",
    "        out = out_f.view(out_f.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out_f,out\n",
    "\n",
    "base_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "model = NIMA(base_model)\n",
    "model.load_state_dict(torch.load(os.path.join(os.getcwd(), 'model.pth'), map_location=torch.device('cpu')))\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "res_list = []\n",
    "mean, std = 0.0, 0.0\n",
    "tic = time.perf_counter()\n",
    "for img in img_list:\n",
    "    im = Image.open(os.path.join(image_path, str(img))).convert('RGB')\n",
    "    imt = test_transform(im)\n",
    "    imt = imt.unsqueeze(dim=0)\n",
    "    imt = imt.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_f, out_class = model(imt)\n",
    "    out_class = out_class.view(10, 1)\n",
    "    for j, e in enumerate(out_class, 1):\n",
    "        mean += j * e\n",
    "    for k, e in enumerate(out_class, 1):\n",
    "        std += e * (k - mean) ** 2\n",
    "    std = std ** 0.5\n",
    "    res_list.append((float(mean),float(std)))\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME - {toc - tic:0.2f} s\")\n",
    "print(img_list)\n",
    "print(res_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-23T12:41:04.675566599Z",
     "start_time": "2023-09-23T12:40:58.770608368Z"
    }
   },
   "id": "34f751bf0ccb4f14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ef5f0a82ec3f1f08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
