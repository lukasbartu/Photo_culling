{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Image Quality\n",
    "- Brisque - https://pypi.org/project/brisque/\n",
    "- image-quality 1.2.7 - https://pypi.org/project/image-quality/\n",
    "- NIMA for aesthetic quality - https://github.com/yunxiaoshi/Neural-IMage-Assessment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f786a6e7de7ade7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For assessing image quality we have to consider two aspects. Technical and aesthetic quality of an image. Our usage involves no reference for quality assessment. We are going to look into how fast and sensitive the image quality evaluator is and evaluate which one is the most efficient for our use.\n",
    "\n",
    "For testing we created small folder with image and its various augmentations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26d94ea8f3d13875"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import time,os\n",
    "import torchvision.transforms as transforms\n",
    "# We create list of images that we use for testing\n",
    "\n",
    "image_path = '/home/lukas/Bakalářka/photo_culling/images/testing'\n",
    "img_list = []  # list of image file names to process\n",
    "for path in os.scandir(image_path):\n",
    "    if path.is_file():\n",
    "        if path.name.endswith(\".jpg\"):\n",
    "            img_list += [path.name]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T12:58:46.395440795Z",
     "start_time": "2023-09-24T12:58:46.386165315Z"
    }
   },
   "id": "e30cf9e3f73ce306"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TECHNICAL QUALITY\n",
    "Firstly we are going to focus on technical quality. For technical quality we can choose from two approaches, one that is algorithmic and focused on scene statistics and the second one, which is using CNN trained on TID2013 dataset.\n",
    "\n",
    "For algorithmic approach we tested two implementations of the same algorithm called BRISQUE (Blind/referenceless image spatial quality evaluator). First implementation is library created by Rehan Guha (https://pypi.org/project/brisque/). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b81d75e55fe9ab"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME - 53.66 s\n",
      "['clear.jpg', 'GaussBlur.jpg', 'rotated.jpg', 'inverted.jpg', 'hue_shift.jpg', 'contrast.jpg']\n",
      "[63.439140291231155, 96.60843292281524, 63.230106082455876, 63.30271773240301, 59.23486617151738, 71.03791634023887]\n"
     ]
    }
   ],
   "source": [
    "from brisque import BRISQUE\n",
    "from skimage import io\n",
    "\n",
    "obj = BRISQUE(url=False)\n",
    "results_BRISQUE = []\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for img in img_list:\n",
    "    x = io.imread(os.path.join(image_path,img))\n",
    "    results_BRISQUE.append(obj.score(x))\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME - {toc - tic:0.2f} s\")\n",
    "print(img_list)\n",
    "print(results_BRISQUE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T12:59:42.524038352Z",
     "start_time": "2023-09-24T12:58:48.842736246Z"
    }
   },
   "id": "85cbdd61b53f35d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From this first test we can see that for this algorithm rotating and inverting has little to no effect. For shifted hue and increased contrast we can observe\n",
    "small change in quality score. And for Gaussian blur we can see the biggest quality score drop. This is expected as the technical quality should only be measured\n",
    "by pixels relation to its surroundings.\n",
    "\n",
    "Now we can try testing the second implementation of BRISQUE from image-quality library made by Ricardo Ocampo. In this implementation we need to open the images\n",
    "with Pillow image library function. This is slight downside."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2adfee4132c43ef6"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [28], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m img_list:\n\u001B[1;32m      8\u001B[0m     x \u001B[38;5;241m=\u001B[39m PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mopen(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(image_path,img))\n\u001B[0;32m----> 9\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mimquality\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbrisque\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     results_BRISQUE\u001B[38;5;241m.\u001B[39mappend(x)\n\u001B[1;32m     11\u001B[0m toc \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/imquality/brisque.py:161\u001B[0m, in \u001B[0;36mscore\u001B[0;34m(image, kernel_size, sigma)\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mscore\u001B[39m(image: PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7\u001B[39m, sigma\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m6\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m--> 161\u001B[0m     scaled_features \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m predict(scaled_features)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/imquality/brisque.py:146\u001B[0m, in \u001B[0;36mcalculate_features\u001B[0;34m(image, kernel_size, sigma)\u001B[0m\n\u001B[1;32m    137\u001B[0m     downscaled_image \u001B[38;5;241m=\u001B[39m skimage\u001B[38;5;241m.\u001B[39mtransform\u001B[38;5;241m.\u001B[39mrescale(\n\u001B[1;32m    138\u001B[0m         brisque\u001B[38;5;241m.\u001B[39mimage,\n\u001B[1;32m    139\u001B[0m         \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    143\u001B[0m         \u001B[38;5;66;03m# multichannel=False,\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     )\n\u001B[1;32m    145\u001B[0m downscaled_brisque \u001B[38;5;241m=\u001B[39m Brisque(downscaled_image, kernel_size\u001B[38;5;241m=\u001B[39mkernel_size, sigma\u001B[38;5;241m=\u001B[39msigma)\n\u001B[0;32m--> 146\u001B[0m features \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39mconcatenate([\u001B[43mbrisque\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures\u001B[49m, downscaled_brisque\u001B[38;5;241m.\u001B[39mfeatures])\n\u001B[1;32m    147\u001B[0m scaled_features \u001B[38;5;241m=\u001B[39m scale_features(features)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m scaled_features\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/imquality/brisque.py:93\u001B[0m, in \u001B[0;36mBrisque.features\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeatures\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m numpy\u001B[38;5;241m.\u001B[39mconcatenate(\n\u001B[0;32m---> 93\u001B[0m         [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculate_features(mscn_type) \u001B[38;5;28;01mfor\u001B[39;00m mscn_type \u001B[38;5;129;01min\u001B[39;00m MscnType]\n\u001B[1;32m     94\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/imquality/brisque.py:93\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeatures\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m numpy\u001B[38;5;241m.\u001B[39mconcatenate(\n\u001B[0;32m---> 93\u001B[0m         [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcalculate_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmscn_type\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m mscn_type \u001B[38;5;129;01min\u001B[39;00m MscnType]\n\u001B[1;32m     94\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/imquality/brisque.py:119\u001B[0m, in \u001B[0;36mBrisque.calculate_features\u001B[0;34m(self, mscn_type)\u001B[0m\n\u001B[1;32m    109\u001B[0m     var \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39mmean(\n\u001B[1;32m    110\u001B[0m         [numpy\u001B[38;5;241m.\u001B[39msquare(agg\u001B[38;5;241m.\u001B[39msigma_left), numpy\u001B[38;5;241m.\u001B[39msquare(agg\u001B[38;5;241m.\u001B[39msigma_right)]\n\u001B[1;32m    111\u001B[0m     )\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m numpy\u001B[38;5;241m.\u001B[39marray([agg\u001B[38;5;241m.\u001B[39malpha, var])\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m numpy\u001B[38;5;241m.\u001B[39marray(\n\u001B[1;32m    115\u001B[0m     [\n\u001B[1;32m    116\u001B[0m         agg\u001B[38;5;241m.\u001B[39malpha,\n\u001B[1;32m    117\u001B[0m         agg\u001B[38;5;241m.\u001B[39mmean,\n\u001B[1;32m    118\u001B[0m         numpy\u001B[38;5;241m.\u001B[39msquare(agg\u001B[38;5;241m.\u001B[39msigma_left),\n\u001B[0;32m--> 119\u001B[0m         numpy\u001B[38;5;241m.\u001B[39msquare(\u001B[43magg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msigma_right\u001B[49m),\n\u001B[1;32m    120\u001B[0m     ]\n\u001B[1;32m    121\u001B[0m )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/imquality/statistics.py:60\u001B[0m, in \u001B[0;36mAsymmetricGeneralizedGaussian.sigma_right\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msigma_right\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sigma\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDistributionSide\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mright\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/imquality/statistics.py:46\u001B[0m, in \u001B[0;36mAsymmetricGeneralizedGaussian._sigma\u001B[0;34m(self, side)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sigma\u001B[39m(\u001B[38;5;28mself\u001B[39m, side: DistributionSide):\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m side \u001B[38;5;241m==\u001B[39m DistributionSide\u001B[38;5;241m.\u001B[39mright:\n\u001B[0;32m---> 46\u001B[0m         _x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx_right\u001B[49m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m side \u001B[38;5;241m==\u001B[39m DistributionSide\u001B[38;5;241m.\u001B[39mleft:\n\u001B[1;32m     48\u001B[0m         _x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_left\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/imquality/statistics.py:42\u001B[0m, in \u001B[0;36mAsymmetricGeneralizedGaussian.x_right\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mx_right\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_x\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDistributionSide\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mright\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import imquality.brisque\n",
    "\n",
    "results_BRISQUE = []\n",
    "\n",
    "tic = time.perf_counter()\n",
    "for img in img_list:\n",
    "    x = PIL.Image.open(os.path.join(image_path,img))\n",
    "    x = imquality.brisque.score(x)\n",
    "    results_BRISQUE.append(x)\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME - {toc - tic:0.2f} s\")\n",
    "print(img_list)\n",
    "print(results_BRISQUE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T12:44:11.171586054Z",
     "start_time": "2023-09-24T12:40:18.511365095Z"
    }
   },
   "id": "5263bb5080c34aa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the second test we can observe the same as in the first one. The most significant score change is in the case of blurring the image. The quality scores\n",
    "for both implementations are nearly identical.\n",
    "\n",
    "Both of these solutions are very easy-to-use, give us single score number representing the technical quality and are linear in terms of computing time. That said,\n",
    "the first implementation is around 5 times faster and doesn't require us to open images beforehand. From these factors we can already see that the first\n",
    "implementation is superior of those two.\n",
    "\n",
    "Now we can go ahead and test CNN approach to technical quality assessment.\n",
    "- For this I haven't been able to test it yet. As I struggle to build the network with pretrained weights."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0bc1cd2a4e9319"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T12:59:56.950324265Z",
     "start_time": "2023-09-24T12:59:56.919040202Z"
    }
   },
   "id": "9fe8adc961934356"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AESTHETIC QUALITY\n",
    "For aesthetics there are significantly fewer solutions as the question of aesthetics is vey subjective and so it's complicated to create an algorithmic solution.\n",
    "For this reason we are going to focus on deep learning approach with CNN trained on AVA dataset. This dataset is created with images of amateur photographers and as such they are focused on aesthetic quality to the images. The rating is trained on ratings of the public and hence it is as close to objective beauty rating as we can currently get. The output of the network is distribution of ratings that is simulating an actual distribution of ratings that people might give. From this distribution we are then able to get mean rating and the statistical deviation of the rating.\n",
    "\n",
    "We used implementation inspired by https://github.com/yunxiaoshi/Neural-IMage-Assessment. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afee0aca3102d95e"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME - 2.74 s\n",
      "['clear.jpg', 'GaussBlur.jpg', 'rotated.jpg', 'inverted.jpg', 'hue_shift.jpg', 'contrast.jpg']\n",
      "[(6.24, 1.21), (5.53, 1.47), (6.13, 1.32), (6.35, 1.49), (6.21, 1.27), (5.93, 1.37)]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NIMA(nn.Module):\n",
    "    \"\"\"Neural IMage Assessment model by Google\"\"\"\n",
    "    def __init__(self, base_model, num_classes=10):\n",
    "        super(NIMA, self).__init__()\n",
    "        self.features = base_model.features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.75),\n",
    "            nn.Linear(in_features=25088, out_features=num_classes),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_f = self.features(x)\n",
    "        out = out_f.view(out_f.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out_f,out\n",
    "\n",
    "base_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "model = NIMA(base_model)\n",
    "model.load_state_dict(torch.load(os.path.join(os.getcwd(), 'model.pth'), map_location=torch.device('cpu')))\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "            \n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "res_list = []\n",
    "mean, std = 0.0, 0.0\n",
    "tic = time.perf_counter()\n",
    "for img in img_list:\n",
    "    im = Image.open(os.path.join(image_path, str(img))).convert('RGB')\n",
    "    imt = test_transform(im)\n",
    "    imt = imt.unsqueeze(dim=0)\n",
    "    imt = imt.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_f, out_class = model(imt)\n",
    "    out_class = out_class.view(10, 1)\n",
    "    for j, e in enumerate(out_class, 1):\n",
    "        mean += j * e\n",
    "    for k, e in enumerate(out_class, 1):\n",
    "        std += e * (k - mean) ** 2\n",
    "    std = std ** 0.5\n",
    "    mean = int(mean.item()*100)/100\n",
    "    std = int(std.item()*100)/100\n",
    "    res_list.append((mean,std))\n",
    "    mean, std = 0.0, 0.0\n",
    "toc = time.perf_counter()\n",
    "print(f\"TIME - {toc - tic:0.2f} s\")\n",
    "print(img_list)\n",
    "print(res_list)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T13:00:46.833239189Z",
     "start_time": "2023-09-24T13:00:41.592245393Z"
    }
   },
   "id": "34f751bf0ccb4f14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the results we can see that inverting and changing the hue "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c99a6d1cb0ce579"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
